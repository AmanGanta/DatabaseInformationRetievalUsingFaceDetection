{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1102f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_10.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_10.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_10.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_10.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_8.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_8.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_8.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_8.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_8.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_new-beard-look-image-of-ram-charan.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_new-beard-look-image-of-ram-charan.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_new-beard-look-image-of-ram-charan.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_new-beard-look-image-of-ram-charan.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\Ram Charan\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_Ram_Charan_new-beard-look-image-of-ram-charan.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\virat\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_virat_fb5fddcf3206b72e805c6f4b33f5ee6e.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\virat\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_virat_fb5fddcf3206b72e805c6f4b33f5ee6e.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\virat\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_virat_fb5fddcf3206b72e805c6f4b33f5ee6e.jpg due to empty or None face_img.\n",
      "Skipping writing for D:\\projects\\Main projects gec\\face recognition\\Newdata\\virat\\D_projects_Main_projects_gec_face_recognition_face_recognition_data_virat_fb5fddcf3206b72e805c6f4b33f5ee6e.jpg due to empty or None face_img.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from werkzeug.utils import secure_filename\n",
    "import cv2\n",
    "import numpy as np\n",
    "import cvlib as cv\n",
    "\n",
    "def load_and_resize_image(face_img):\n",
    "    p = Image.fromarray(face_img)\n",
    "    r = p.resize((128, 128))\n",
    "    return np.array(r)\n",
    "\n",
    "def normalize_image(image_array):\n",
    "    n = image_array / 255.0\n",
    "    return (n ** 1.5) * 255.0\n",
    "\n",
    "def clip_image_values(image_array):\n",
    "    return np.clip(image_array, 0, 255).astype(np.uint8)\n",
    "\n",
    "def convert_to_gray(image_array):\n",
    "    return cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def denoise_image(gray_image):\n",
    "    return cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "def process_image(face_img):\n",
    "    r = load_and_resize_image(face_img)\n",
    "    n = normalize_image(r)\n",
    "    c = clip_image_values(n)\n",
    "    g = convert_to_gray(c)\n",
    "    d = denoise_image(g)\n",
    "    return d\n",
    "\n",
    "for img in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\face recognition data/*\"):\n",
    "    for i in glob.glob(r\"\" + str(img) + \"/*\"):\n",
    "        f = cv2.imread(i)\n",
    "        faces, confidences = cv.detect_face(f)\n",
    "        os.makedirs(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata2\"+str(img[68:]), exist_ok=True)\n",
    "        basepath = os.path.dirname(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata2\"+str(img[68:]))\n",
    "        for idx, face in enumerate(faces):\n",
    "            (startX, startY, endX, endY) = face\n",
    "            face_img = f[startY:endY, startX:endX]\n",
    "            file_path = os.path.join(\n",
    "                basepath, str(img[69:]), secure_filename(i))\n",
    "            if face_img is not None and face_img.size > 0:\n",
    "                processed_image = process_image(face_img)\n",
    "                cv2.imwrite(file_path, cv2.cvtColor(processed_image, cv2.COLOR_GRAY2RGB))\n",
    "            else:\n",
    "                print(f\"Skipping writing for {file_path} due to empty or None face_img.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3815e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93efbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff87020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9873e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee696df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f702b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    r, c, _ = image.shape\n",
    "    re = cv2.getRotationMatrix2D((c / 2, r / 2), angle, 1)\n",
    "    ro = cv2.warpAffine(image, re, (c, r))\n",
    "    return ro\n",
    "\n",
    "def flip_image_horizontal(image):\n",
    "    f = cv2.flip(image, 1)\n",
    "    return f\n",
    "\n",
    "def adjust_brightness(image, factor):\n",
    "    h = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h[:,:,2] = np.clip(h[:,:,2] * factor, 0, 255)\n",
    "    b = cv2.cvtColor(h, cv2.COLOR_HSV2BGR)\n",
    "    return b\n",
    "\n",
    "def random_crop(image, crop_size):\n",
    "    r, c, _ = image.shape\n",
    "    x = np.random.randint(0, c - crop_size[1] + 1)\n",
    "    y = np.random.randint(0, r - crop_size[0] + 1)\n",
    "    c = image[y:y+crop_size[0], x:x+crop_size[1]]\n",
    "    return cr\n",
    "\n",
    "def random_scale(image, scale_range):\n",
    "    s = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    sc = cv2.resize(image, None, fx=s, fy=s)\n",
    "    return sc\n",
    "\n",
    "def add_noise(image, noise_level):\n",
    "    n = np.random.normal(0, noise_level, image.shape)\n",
    "    no = np.clip(image + noise, 0, 255)\n",
    "    return no\n",
    "\n",
    "def augment_and_save_images(input_folder, output_folder, max_output_images=50):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    fe = os.listdir(input_folder)\n",
    "    random.shuffle(fe)\n",
    "    g = 0\n",
    "    for f in fe:\n",
    "        if g >= max_output_images:\n",
    "            break\n",
    "        im = os.path.join(input_folder, f)\n",
    "        image = cv2.imread(im)\n",
    "        for i in range(2):\n",
    "            au = random.choice(['rotate', 'flip', 'brightness'])\n",
    "            if au == 'rotate':\n",
    "                rotated_img = rotate_image(image, np.random.uniform(-30, 30))\n",
    "                augmented_img = rotated_img\n",
    "            elif au == 'flip':\n",
    "                if np.random.rand() > 0.5:\n",
    "                    flipped_img = flip_image_horizontal(image)\n",
    "                    augmented_img = flipped_img\n",
    "                else:\n",
    "                    augmented_img = image\n",
    "            elif au == 'brightness':\n",
    "                brightness_factor = np.random.uniform(0.5, 1.5)\n",
    "                brightened_img = adjust_brightness(image, brightness_factor)\n",
    "                augmented_img = brightened_img\n",
    "            output_filename = f\"aug_{g}_{au}_{f}\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cv2.imwrite(output_path, augmented_img)\n",
    "            g += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddda2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8065db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for img in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata/*\"):\n",
    "    input_folder = img\n",
    "    output_folder =img \n",
    "    augment_and_save_images(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60a2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_folder_to_new_directory(old_folder_path, new_folder_name):\n",
    "    new_folder_path = os.path.join(os.path.dirname(old_folder_path), new_folder_name)\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "    shutil.move(old_folder_path, new_folder_path)\n",
    "    print(f\"Folder '{os.path.basename(old_folder_path)}' moved to '{new_folder_path}'\")\n",
    "for i in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata/*\"):\n",
    "    new_folder_name = \"x\"+i[55:]\n",
    "    move_folder_to_new_directory(i, new_folder_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efecfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad709d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c0c8b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 153 images belonging to 1 classes.\n",
      "Found 153 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amang\\AppData\\Local\\Temp\\ipykernel_17308\\2322451824.py:34: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 33s 8s/step - loss: 8.6980e-10 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 45s 10s/step - loss: 1.0678e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 44s 10s/step - loss: 1.2857e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 45s 10s/step - loss: 1.2060e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 48s 11s/step - loss: 1.1123e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 46s 10s/step - loss: 1.2433e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 1.2036e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 48s 11s/step - loss: 1.1362e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 49s 11s/step - loss: 8.3433e-10 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 51s 12s/step - loss: 1.0994e-09 - accuracy: 1.0000 - val_loss: 1.0451e-09 - val_accuracy: 1.0000\n",
      "Found 144 images belonging to 1 classes.\n",
      "Found 144 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 3.3660e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 50s 12s/step - loss: 3.1625e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 3.1838e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 3.2370e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 2.9765e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 50s 12s/step - loss: 2.8128e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 2.9818e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 2.7100e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 2.7569e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 50s 11s/step - loss: 2.7503e-09 - accuracy: 1.0000 - val_loss: 2.2770e-09 - val_accuracy: 1.0000\n",
      "Found 135 images belonging to 1 classes.\n",
      "Found 135 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.9411e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.6129e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.3593e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 3.0070e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.8211e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 47s 11s/step - loss: 2.4797e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.4785e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 52s 13s/step - loss: 2.9268e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 42s 9s/step - loss: 2.5663e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 47s 10s/step - loss: 2.5893e-09 - accuracy: 1.0000 - val_loss: 2.5254e-09 - val_accuracy: 1.0000\n",
      "Found 89 images belonging to 1 classes.\n",
      "Found 89 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 27s 11s/step - loss: 3.2704e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 32s 13s/step - loss: 4.1267e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 37s 15s/step - loss: 3.3392e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 33s 13s/step - loss: 3.2519e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 31s 13s/step - loss: 3.6179e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 32s 13s/step - loss: 3.5650e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 33s 13s/step - loss: 3.4894e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 31s 12s/step - loss: 3.3065e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 32s 13s/step - loss: 3.6786e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 32s 13s/step - loss: 4.1024e-09 - accuracy: 1.0000 - val_loss: 3.8556e-09 - val_accuracy: 1.0000\n",
      "Found 99 images belonging to 1 classes.\n",
      "Found 99 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 35s 11s/step - loss: 2.1943e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 36s 10s/step - loss: 2.2125e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 35s 10s/step - loss: 1.5763e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 36s 12s/step - loss: 1.9589e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 36s 10s/step - loss: 2.1137e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 36s 12s/step - loss: 1.8430e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 35s 10s/step - loss: 1.6045e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 35s 10s/step - loss: 2.2027e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 35s 10s/step - loss: 2.3315e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 35s 10s/step - loss: 2.1921e-10 - accuracy: 1.0000 - val_loss: 2.0914e-10 - val_accuracy: 1.0000\n",
      "Found 81 images belonging to 1 classes.\n",
      "Found 81 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 29s 12s/step - loss: 3.7530e-09 - accuracy: 1.0000 - val_loss: 4.4292e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 5.7858e-09 - accuracy: 1.0000 - val_loss: 4.4292e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 5.7608e-09 - accuracy: 1.0000 - val_loss: 4.4292e-09 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 4.5304e-09 - accuracy: 1.0000 - val_loss: 4.4292e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 4.3390e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 3.4168e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 3.8708e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 5.2061e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 3.8883e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 4.9727e-09 - accuracy: 1.0000 - val_loss: 4.4291e-09 - val_accuracy: 1.0000\n",
      "Found 81 images belonging to 1 classes.\n",
      "Found 81 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 28s 12s/step - loss: 1.2765e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 1.2746e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 1.0484e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 31s 12s/step - loss: 1.1189e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 1.3839e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 1.1860e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 1.3145e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 9.8428e-10 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 1.2732e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 29s 13s/step - loss: 1.4413e-09 - accuracy: 1.0000 - val_loss: 1.1392e-09 - val_accuracy: 1.0000\n",
      "Found 144 images belonging to 1 classes.\n",
      "Found 144 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 51s 11s/step - loss: 4.8477e-09 - accuracy: 1.0000 - val_loss: 7.0217e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 51s 11s/step - loss: 5.1199e-09 - accuracy: 1.0000 - val_loss: 7.0217e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 51s 11s/step - loss: 7.5501e-09 - accuracy: 1.0000 - val_loss: 7.0217e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 51s 11s/step - loss: 4.0118e-09 - accuracy: 1.0000 - val_loss: 7.0216e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 52s 11s/step - loss: 5.9836e-09 - accuracy: 1.0000 - val_loss: 7.0216e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 54s 12s/step - loss: 5.4786e-09 - accuracy: 1.0000 - val_loss: 7.0216e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 52s 11s/step - loss: 4.9119e-09 - accuracy: 1.0000 - val_loss: 7.0215e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 48s 11s/step - loss: 6.6252e-09 - accuracy: 1.0000 - val_loss: 7.0215e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 46s 10s/step - loss: 3.0016e-09 - accuracy: 1.0000 - val_loss: 7.0215e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 45s 10s/step - loss: 4.2850e-09 - accuracy: 1.0000 - val_loss: 7.0215e-09 - val_accuracy: 1.0000\n",
      "Found 81 images belonging to 1 classes.\n",
      "Found 81 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 25s 10s/step - loss: 1.5630e-08 - accuracy: 1.0000 - val_loss: 1.4921e-08 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 26s 11s/step - loss: 1.5869e-08 - accuracy: 1.0000 - val_loss: 1.4921e-08 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 26s 10s/step - loss: 1.5812e-08 - accuracy: 1.0000 - val_loss: 1.4921e-08 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 27s 12s/step - loss: 1.4064e-08 - accuracy: 1.0000 - val_loss: 1.4921e-08 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 27s 12s/step - loss: 1.6344e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 28s 11s/step - loss: 1.6560e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 28s 11s/step - loss: 1.3620e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 29s 11s/step - loss: 1.6817e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 28s 11s/step - loss: 1.6683e-08 - accuracy: 1.0000 - val_loss: 1.4920e-08 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 27s 12s/step - loss: 1.5253e-08 - accuracy: 1.0000 - val_loss: 1.4919e-08 - val_accuracy: 1.0000\n",
      "Found 233 images belonging to 1 classes.\n",
      "Found 233 images belonging to 1 classes.\n",
      "Error: [Errno 2] No such file or directory: 'D:\\\\projects\\\\Main projects gec\\\\face recognition\\\\Newdata\\\\xSamantha\\\\Samantha\\\\aug_142_rotate_aug_19_flip_D_projects_Main_projects_gec_face_recognition_face_recognition_data_Samantha_samantha-akkineni-beautiful-hd-photos-mobile-wallpapers-hd-androidiphone-t9iv-lg.jpg'\n",
      "Found 81 images belonging to 1 classes.\n",
      "Found 81 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 26s 11s/step - loss: 6.7699e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 26s 10s/step - loss: 7.3898e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 26s 10s/step - loss: 1.0611e-09 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 27s 10s/step - loss: 7.0249e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 27s 11s/step - loss: 1.2025e-09 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 40s 17s/step - loss: 6.3963e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 30s 10s/step - loss: 8.3768e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 27s 10s/step - loss: 8.1865e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 26s 10s/step - loss: 7.7698e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 27s 11s/step - loss: 7.1731e-10 - accuracy: 1.0000 - val_loss: 1.0394e-09 - val_accuracy: 1.0000\n",
      "Found 117 images belonging to 1 classes.\n",
      "Found 117 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 39s 11s/step - loss: 5.9253e-09 - accuracy: 1.0000 - val_loss: 5.0598e-09 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 40s 11s/step - loss: 6.6807e-09 - accuracy: 1.0000 - val_loss: 5.0597e-09 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 41s 12s/step - loss: 6.3790e-09 - accuracy: 1.0000 - val_loss: 5.0595e-09 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 46s 13s/step - loss: 6.5225e-09 - accuracy: 1.0000 - val_loss: 5.0594e-09 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 42s 12s/step - loss: 7.0520e-09 - accuracy: 1.0000 - val_loss: 5.0592e-09 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 40s 11s/step - loss: 6.1831e-09 - accuracy: 1.0000 - val_loss: 5.0590e-09 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 36s 10s/step - loss: 6.5055e-09 - accuracy: 1.0000 - val_loss: 5.0587e-09 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 38s 11s/step - loss: 7.3358e-09 - accuracy: 1.0000 - val_loss: 5.0585e-09 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 38s 11s/step - loss: 6.7191e-09 - accuracy: 1.0000 - val_loss: 5.0583e-09 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 43s 12s/step - loss: 4.8853e-09 - accuracy: 1.0000 - val_loss: 5.0581e-09 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "for i in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata/*\"):\n",
    "    training_set = train_datagen.flow_from_directory(\n",
    "        i,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    test_set = test_datagen.flow_from_directory(\n",
    "        i,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        r = model.fit_generator(\n",
    "            training_set,\n",
    "            validation_data=test_set,\n",
    "            epochs=10,\n",
    "            steps_per_epoch=len(training_set),\n",
    "            validation_steps=len(test_set)\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue  \n",
    "    \n",
    "    name = i[55:]\n",
    "    model_path = os.path.join(\n",
    "        r\"D:\\projects\\Main projects gec\\face recognition\\models\", f'{name}.h5')\n",
    "    model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db42cbcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4fc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940fe24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25d5ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "try:\n",
    "    # Connect to the SQLite database (or create a new one if it doesn't exist)\n",
    "    conn = sqlite3.connect(r'verify_data.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create a table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS users (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            pickle_data BLOB,\n",
    "            name TEXT,\n",
    "            username TEXT,\n",
    "            password TEXT,\n",
    "            details TEXT\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(\"SQLite error:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f6daed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=\"D:\\projects\\Main projects gec\\face recognition\\models\"\n",
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3f1e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "for i in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\models/*\"):\n",
    "    conn = sqlite3.connect('verify_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    with open(i, 'rb') as file:\n",
    "        h5_data = file.read()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO users (pickle_data, name, username, password, details)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (sqlite3.Binary(h5_data),i[53:],i[53:]+\"123\", 'password123', 'Additional details'))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5956504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ac2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6419f532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea9d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad55c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 170ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 313ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 321ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 312ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 289ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 260ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 269ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 253ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 242ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 270ms/step\n",
      "[[1.]]\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "for i in glob.glob(r\"D:\\projects\\Main projects gec\\face recognition\\Newdata\\xSamantha\\Samantha/*\"):\n",
    "    face_crop = cv2.imread(i)\n",
    "    face_crop = cv2.resize(face_crop, (224,224))\n",
    "    face_crop = face_crop.astype(\"float\") / 255.0\n",
    "    face_crop = img_to_array(face_crop)\n",
    "    face_crop = np.expand_dims(face_crop, axis=0)\n",
    "    p = model.predict(face_crop)\n",
    "    print(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77af8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365cdac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
